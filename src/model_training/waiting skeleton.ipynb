{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kynJQfbnbYjG"
      },
      "source": [
        "https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=8DKMc0fiej4N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dec36z4GYpRe"
      },
      "source": [
        "# Настройка доступов и подгрузка библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHRMYs8D-kVi"
      },
      "source": [
        "meme_pattern = 'waiting skeleton'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMcA-r-JYwhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d0ffde-ac54-4add-90ed-e7202bb9febc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo0knl7RbaP7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igcl3kHaCZY5"
      },
      "source": [
        "!ln -s \"/content/drive/My Drive\" /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "733q7zPFaBSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deaad33f-8e6e-46e0-82c9-02f8a522b9f2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install git+https://github.com/SerSmith/gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting git+https://github.com/SerSmith/gpt-2-simple\n",
            "  Cloning https://github.com/SerSmith/gpt-2-simple to /tmp/pip-req-build-fybvb7og\n",
            "  Running command git clone -q https://github.com/SerSmith/gpt-2-simple /tmp/pip-req-build-fybvb7og\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.18.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->gpt-2-simple==0.7.1) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->gpt-2-simple==0.7.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->gpt-2-simple==0.7.1) (0.17.0)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=25644 sha256=891a8d726b2a18d0dac0cfe1b0acf4d2de6c9b8f2093a5a5e7d878bc612889b8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2_ew3y07/wheels/05/e5/9d/9b6ebf15d32922867291a66ba17b8610a5621e6cd1a399fba5\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scJH4yBGUKTn"
      },
      "source": [
        "# Подготовка датасета для дообучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWJvqxY11oFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537655f4-bc3d-4804-937f-8c0249b066c7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "import shutil\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "path_raw_data = '/gdrive/MADE_PROJECT/raw_data/memes_all_eng_v2.csv'\n",
        "path_folder_gdisk = '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS6A3cQATkdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c258a3-ece8-44cc-fa0f-8825d23689d9"
      },
      "source": [
        "df = pd.read_csv(path_raw_data)\n",
        "df['template_name_prep'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "batman slapping robin    18241\n",
              "x x everywhere           16677\n",
              "waiting skeleton         12314\n",
              "change my mind           12169\n",
              "disaster girl             9418\n",
              "Name: template_name_prep, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7rtIwaDdqUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9936134-f3dc-4b86-b330-81a7185d1211"
      },
      "source": [
        "def prepare_text(text):\n",
        "  text = str(text).strip().upper()\n",
        "  text = text.replace(r\"\\s\", \" \")\n",
        "  text = text.replace(r\"\\n\", \" \")\n",
        "  text = text.replace(r\"&\", \"and\")\n",
        "  text = text.rstrip('|')\n",
        "  return text\n",
        "\n",
        "def prepare_train(data, template, dest_path, min_size=10):\n",
        "  out = df[df['template_name_prep'] == template]['text'].drop_duplicates().reset_index(drop=True).apply(prepare_text)\n",
        "  out = out[out.apply(len) >= min_size ]\n",
        "  print(out.shape)\n",
        "  out.to_csv(dest_path, index=False, header=False)\n",
        "  return out\n",
        "\n",
        "q = prepare_train(df, meme_pattern, os.path.join(path_folder_gdisk, f'{meme_pattern}.csv'))\n",
        "table = os.path.join(path_folder_gdisk, f'{meme_pattern}.csv')\n",
        "!cat \"$table\" | head -n 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12220,)\n",
            "WAITING|FOR THE BEAT TO DROP\n",
            "ME WAITING ON|A BBC CORONA MEME\n",
            "\"TEACHERS WHEN THEY SAY|\"\"IMMA WAIT TILL IT QUIET\"\"\"\n",
            "|FINDING OUT MY LIKE GREAT-GRANDMA GOT MARRIED AT 14 TO A 50 YEAR OLD MAN\n",
            "PEOPLE WAITING FOR|THE LOCKDOWN TO END\n",
            "|JUST WAITING FOR CAM TO LOAD\n",
            "ME WAITING FOR SOME SORT OF GUIDANCE\n",
            "|WAITING FOR SCHOOL TO REOPEN BE LIKE\n",
            "I'VE BEEN WAITING FOREVER!\n",
            "ADA COUNTY EMPLOYEES WAITING|FOR SURFACE PRO TO RESTART\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCjVlXXVIHo"
      },
      "source": [
        "Запуск дообучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfenTwWl_dqw"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_tO82mt-b_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03173f60-8ca7-4419-cc34-b6c1e9e960bd"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Nov 28 15:08:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfjBFBH0aFQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7981b9bf-d328-4dfd-a383-505820582396"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.mount_gdrive()\n",
        "gpt2.download_gpt2(model_name=\"124M\")\n",
        "gpt2.copy_file_from_gdrive(f'{meme_pattern}.csv')\n",
        "\n",
        "history = gpt2.finetune(sess,\n",
        "        f'{meme_pattern}.csv',\n",
        "        steps=2000,\n",
        "        model_name='124M',\n",
        "        model_dir='models',\n",
        "        combine=50000,\n",
        "        batch_size=1,\n",
        "        learning_rate=0.0001,\n",
        "        accumulate_gradients=5,\n",
        "        restore_from='latest',\n",
        "        run_name=meme_pattern,\n",
        "        checkpoint_dir='checkpoint',\n",
        "        sample_every=50,\n",
        "        sample_length=50,\n",
        "        sample_num=10,\n",
        "        multi_gpu=False,\n",
        "        save_every=1000,\n",
        "        print_every=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 295Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 134Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 435Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001:   0%|                          | 0.00/498M [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 200Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 461Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 155Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 197Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 61.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 394043 tokens\n",
            "Training...\n",
            "[50 | 124.13] loss=1.57 avg=1.57\n",
            " METE 4 SEASON 4 \"DONE IN THE DARK\" SEASON 5 THE BOWL, OR THE GRANDMA OF MATH. TEXTS IN HONEY. SHOWS WHAT HAPPENS TO YOU. YOU'RE ALIVE\n",
            "[100 | 249.43] loss=1.49 avg=1.53\n",
            " FOR YOU, A SORRY PERSON. I DO NOT FEEL BURN. IT'S ME!!!<|endoftext|>\n",
            "<|startoftext|>WAITING FOR YOUR GIRLFRIEND TO SHOP\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[150 | 373.81] loss=1.33 avg=1.46\n",
            "ONE POD<|endoftext|>\n",
            "<|startoftext|>WAKE ME UP THAN I WOULD<|endoftext|>\n",
            "<|startoftext|>ME WAITING FOR THE NEXT SEASON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[200 | 498.34] loss=1.24 avg=1.41\n",
            "ITCHofTOO<|endoftext|>\n",
            "<|startoftext|>WHEN YOUR FRIEND IS WAITING FOR YOU AT THE PARK FOR DURING YOUR WIFE'S BURGELY LIFE<|endoftext\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[250 | 622.49] loss=1.12 avg=1.35\n",
            "text|>WAITING FOR DONALD TRUMP TO STOP BEING A PUNSCH<|endoftext|>\n",
            "<|startoftext|>I JUST HAVE A HONEST TIME. WAITING FOR A GOOD PRESIDENT<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[300 | 746.94] loss=1.09 avg=1.30\n",
            " BIRTHY WAITING FOR HER. TO TELL ME HOW HE FEELS<|endoftext|>\n",
            "<|startoftext|>WHEN YOU WAIT. FOR ANOTHER HUBBORN<|endoftext\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[350 | 871.26] loss=1.08 avg=1.27\n",
            "TH WAITING FOR PEOPLE TO STOP BEING ARCHDATED..<|endoftext|>\n",
            "<|startoftext|>ME WAITING FOR A WOMAN<|endoftext|>\n",
            "<|startoftext\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[400 | 995.36] loss=0.86 avg=1.22\n",
            "|startoftext|>ME WAITING FOR MELODY TO POST A VIDEO OF ME. STILL WAITING<|endoftext|>\n",
            "<|startoftext|>WHEN YOU FINNITE. ALL 4 OF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[450 | 1119.45] loss=0.78 avg=1.17\n",
            " THEN THEN IT GETS FINE AND IT FEELS DEAD INSIDE<|endoftext|>\n",
            "<|startoftext|>MY MOM SUCKED ME OUT OF THE MALL<|endoftext|>\n",
            "<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[500 | 1243.89] loss=0.77 avg=1.13\n",
            "endoftext|>\n",
            "<|startoftext|>WAITING FOR. THE PERFECT WOMAN.<|endoftext|>\n",
            "<|startoftext|>MY SISTER. SAYING ILL GET A CUTE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[550 | 1368.21] loss=0.87 avg=1.10\n",
            " LOVE...<|endoftext|>\n",
            "<|startoftext|>I WAS WAITING. FOR MY CRUSH TO TEXT ME BACK<|endoftext|>\n",
            "<|startoftext|>STILL WAITING\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[600 | 1492.58] loss=0.60 avg=1.06\n",
            "startoftext|>HOW LONG IT TAKES. FOR SOMEONE TO NOTICE THAT THEY FOUND A SANDWICH|HYDROPHECY<|endoftext|>\n",
            "<|startoftext|>THE ONE TIM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[650 | 1616.92] loss=0.58 avg=1.02\n",
            " WAIT FOR. AMERICA TO BE GREAT AGAIN<|endoftext|>\n",
            "<|startoftext|>WAITING AT THE DMV BE LIKE<|endoftext|>\n",
            "<|startoftext|>THIS MEW\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[700 | 1741.32] loss=0.52 avg=0.98\n",
            " THE ALIVE<|endoftext|>\n",
            "<|startoftext|>DIE ON TRAFFIC|HOW COULD IT HAVE HAPPEN<|endoftext|>\n",
            "<|startoftext|>ME SITT\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[750 | 1865.59] loss=0.49 avg=0.94\n",
            " FOR AN XBOX GAME. AND IGNORANT THOSE WHO ENTROLL SPAGHETTI, WHENEVER THESE WEIGHT LOSSES WILL OCCUR TO APPEAR IN A STRICTLY ALONE GROUP. BUT FOR\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[800 | 1989.87] loss=0.38 avg=0.91\n",
            " I TO GET UP.<|endoftext|>\n",
            "<|startoftext|>ME WAITING FOR. DCTV MOVIE<|endoftext|>\n",
            "<|startoftext|>WAITING FOR A HOT G\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[850 | 2114.09] loss=0.45 avg=0.88\n",
            "MEING. FOR THE BUS|IN 2060<|endoftext|>\n",
            "<|startoftext|>WAITING FOR CLASS|TO START<|endoftext|>\n",
            "<|startoftext|>WHEN SCHOOL\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[900 | 2238.21] loss=0.41 avg=0.85\n",
            "WAITE FOR HER TO GET UP AND LEAVE|MEANING I'LL BE WAITING<|endoftext|>\n",
            "<|startoftext|>ME|WAITING TO GET TO 200,000 POINTS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[950 | 2362.59] loss=0.38 avg=0.82\n",
            "oftext|>MOM:JUST WAIT. TILL HER TRAIN I FINISHED<|endoftext|>\n",
            "<|startoftext|>WAITING FOR THAT NEW DVD EVERYTIME NEW YORK IS CLEAN<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1000 | 2486.79] loss=0.25 avg=0.79\n",
            "Saving checkpoint/waiting skeleton/model-1000\n",
            "ING AN AVERAGE. WEEK ON WEATHER, WEEK ON WEATHER, WEEK ON WEATHER<|endoftext|>\n",
            "<|startoftext|>WAITING FOR. GIRL TO GET READY<|endof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1050 | 2613.95] loss=0.36 avg=0.77\n",
            " TROT. I GOT A BUNCH OF CLONES FROM TIM</|endoftext|>\n",
            "<|startoftext|>WHEN YOU TRY. TO GET A BUSH FOR TACKLING SHORTLY<|endof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1100 | 2737.76] loss=0.27 avg=0.74\n",
            ". MY LIFE<|endoftext|>\n",
            "<|startoftext|>WAITING FOR COVID19 TO BE OVER<|endoftext|>\n",
            "<|startoftext|>MEANWHILE. IN A WAR\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1150 | 2861.84] loss=0.16 avg=0.72\n",
            "INGING FOR...|MY BRO|TO SEND CELLPHONE PICS<|endoftext|>\n",
            "<|startoftext|>WHEN YOU|FINISH SCHOOL<|endoftext|>\n",
            "<|start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1200 | 2985.87] loss=0.19 avg=0.69\n",
            " AFTER BED HAS NO BREAK<|endoftext|>\n",
            "<|startoftext|>WAITING FOR YOUR. URGENT PRIZE DIED<|endoftext|>\n",
            "<|startoftext|>O\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1250 | 3110.17] loss=0.11 avg=0.66\n",
            "|>\n",
            "<|startoftext|>ME WAITING TO GET PAID. BE CAREFUL !<|endoftext|>\n",
            "<|startoftext|>ME LISTENING TO! MY KID RATY<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1300 | 3234.47] loss=0.11 avg=0.64\n",
            "|>WAITING ON THE FLOOR OFFICE TO ARRIVE. LIKE..<|endoftext|>\n",
            "<|startoftext|>WAITING FOR DONLAGTER. TO BRING OUR STAR WARS!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1350 | 3358.41] loss=0.12 avg=0.62\n",
            "|STOP SCREEN MAKING EVERY ONE WHO SEES THEM TIRED<|endoftext|>\n",
            "<|startoftext|>WAITING ON THE LEFTIST WEALTH TO SHOWboasts ON THEIR RADIO<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1400 | 3482.47] loss=0.10 avg=0.60\n",
            " TO BE SELF SEARCHING<|endoftext|>\n",
            "<|startoftext|>SOME CASES OF CORONAVIRUS TURN OUT TO BE FAILURES, BUT THESE ARE THE AGE OF INTERNET EX\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1450 | 3606.81] loss=0.12 avg=0.58\n",
            "startoftext|>ME WAITING FOR MY GIRLFREIND|TO MAKE COMMENT ANSWER THE MESSAGE<|endoftext|>\n",
            "<|startoftext|>SATOSHI NAK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1500 | 3730.94] loss=0.08 avg=0.56\n",
            "<|startoftext|>GUY: I WILL WAIT FOR YOU TILL HR TELLS ME WHEN YOU FRESH OUT. ME WHEN I HURRY MY FIRST BODY WEISSER.<|endoftext|>\n",
            "<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1550 | 3854.84] loss=0.08 avg=0.54\n",
            "WAITING FOR A. TRUMP FREE NEWS DAY<|endoftext|>\n",
            "<|startoftext|>WHEN YOUR. MOM SAYS\"BRUH, STOP TALKING SO WE CAN VOTE!\"<|end\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1600 | 3978.99] loss=0.08 avg=0.52\n",
            "endoftext|>\n",
            "<|startoftext|>WAITING FOR SOMEONE TO EAT MY BOX<|endoftext|>\n",
            "<|startoftext|>STILL WAITING FOR TRUMP TO BE IMPEACH\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1650 | 4103.06] loss=0.06 avg=0.51\n",
            "AL CORONA IN THE PARK<|endoftext|>\n",
            "<|startoftext|>I'LL JUST WAIT HERE. UNTIL HALLOWEEN TELLS ME WHAT HAPPENED<|endof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1700 | 4227.28] loss=0.05 avg=0.49\n",
            "|>\n",
            "<|startoftext|>ME WAITING FOR DEVDELLO TO JUST BE A COOL GROUP TO RUN WITH IN THE OLD FORM<|endoftext|>\n",
            "<|startoftext|>WHEN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1750 | 4351.70] loss=0.05 avg=0.48\n",
            " SHIT IT'S 4 O'CLOCK<|endoftext|>\n",
            "<|startoftext|>I GET TIRED SUCKING TROLLS. OUT OF THE BONE<|endoftext|>\n",
            "<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1800 | 4475.71] loss=0.05 avg=0.46\n",
            " FROM YOUR COMPUTER. TO FINISH TALKING ABOUT ANOTHER RAMSH TO END IT ALL. BTW STARTING YOUR MATH ACT NOW THEN.<|endoftext|>\n",
            "<|startoftext|>TEACHERS BE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1850 | 4600.00] loss=0.05 avg=0.45\n",
            "|startoftext|>ME WAITING ON THE|CALL FROM GINSBEW<|endoftext|>\n",
            "<|startoftext|>WAITING TO GET THAT NUMBER<|endoftext|>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1900 | 4724.28] loss=0.05 avg=0.44\n",
            " SHOW WAITING AND HE DIDN'T SAY \"OK THANK YOU\" AND LEFT.<|endoftext|>\n",
            "<|startoftext|>IN LINE FOR LUNCH LIKE. ''<|endoftext|>\n",
            "<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1950 | 4848.51] loss=0.05 avg=0.43\n",
            "|>\n",
            "<|startoftext|>|MY MOM WAITING FOR HER DATE<|endoftext|>\n",
            "<|startoftext|>ME WAITING|FOR A NEW LOYAL CHICK<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2000 | 4972.87] loss=0.04 avg=0.41\n",
            "Saving checkpoint/waiting skeleton/model-2000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI-HwsAN_YXX"
      },
      "source": [
        "with open(f'/gdrive/{meme_pattern}_history.pickle', 'wb') as f:\n",
        "  pickle.dump(history, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UpoKaAHNGzy"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=meme_pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoqIO5KSYR_W"
      },
      "source": [
        "# Генерация текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvbn1BvJYY39"
      },
      "source": [
        "# sess = gpt2.start_tf_sess()\n",
        "# gpt2.load_gpt2(sess, run_name='disaster girl 12 11')\n",
        "# gpt2.generate(sess,\n",
        "#               length=100,\n",
        "#               temperature=0.7,\n",
        "#               prefix=\"Change my mind\",\n",
        "#               nsamples=5,\n",
        "#               batch_size=5,\n",
        "#               truncate='EOF'\n",
        "#               )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}