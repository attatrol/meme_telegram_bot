{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finetune0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kynJQfbnbYjG"
      },
      "source": [
        "https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=8DKMc0fiej4N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dec36z4GYpRe"
      },
      "source": [
        "# Настройка доступов и подгрузка библиотек"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiVKawywOi4H"
      },
      "source": [
        "Введите название темплейта."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHRMYs8D-kVi"
      },
      "source": [
        "meme_pattern = 'disaster girl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMcA-r-JYwhv",
        "outputId": "a74ca191-4fb8-49ff-f44d-be0e1cd4e83d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo0knl7RbaP7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igcl3kHaCZY5"
      },
      "source": [
        "!ln -s \"/content/drive/My Drive\" /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "733q7zPFaBSO",
        "outputId": "55999faf-2c62-4c65-ac3c-66e1ebce492a"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install git+https://github.com/SerSmith/gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting git+https://github.com/SerSmith/gpt-2-simple\n",
            "  Cloning https://github.com/SerSmith/gpt-2-simple to /tmp/pip-req-build-9ixmh8_s\n",
            "  Running command git clone -q https://github.com/SerSmith/gpt-2-simple /tmp/pip-req-build-9ixmh8_s\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.18.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->gpt-2-simple==0.7.1) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->gpt-2-simple==0.7.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->gpt-2-simple==0.7.1) (0.17.0)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=25647 sha256=d0d04da76bace7a1e2df64015e96dc91f1c87f8e6bd6a7b548fa9fa63d832cb7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cb7be4a0/wheels/05/e5/9d/9b6ebf15d32922867291a66ba17b8610a5621e6cd1a399fba5\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scJH4yBGUKTn"
      },
      "source": [
        "# Подготовка датасета для дообучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWJvqxY11oFM",
        "outputId": "a2387ca7-dbb7-450b-eaeb-91a59ceac5da"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "import shutil\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "path_raw_data = '/gdrive/MADE_PROJECT/raw_data/memes_all_eng_v2.csv'\n",
        "path_folder_gdisk = '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS6A3cQATkdM",
        "outputId": "c3e5c672-35ea-45ad-ff19-4f8c47800f09"
      },
      "source": [
        "df = pd.read_csv(path_raw_data)\n",
        "df['template_name_prep'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "batman slapping robin    18241\n",
              "x x everywhere           16677\n",
              "waiting skeleton         12314\n",
              "change my mind           12169\n",
              "disaster girl             9418\n",
              "Name: template_name_prep, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7rtIwaDdqUs",
        "outputId": "ede66cdd-864a-4e3a-e022-4717f87388c8"
      },
      "source": [
        "def prepare_text(text):\n",
        "  text = str(text).strip().upper()\n",
        "  text = re.sub(r\"\\s\", \" \", text)\n",
        "  text = re.sub(r\"\\n\", \" \", text)\n",
        "  text = re.sub(r\"&\", \"and\", text)\n",
        "  text = re.sub(r\"|\", \"\", text)\n",
        "  return text\n",
        "\n",
        "def prepare_train(data, template, dest_path):\n",
        "  out = df[df['template_name_prep'] == template]['text'].drop_duplicates().reset_index(drop=True).apply(prepare_text)\n",
        "  print(out.shape)\n",
        "  out.to_csv(dest_path, index=False, header=False)\n",
        "  return out\n",
        "\n",
        "q = prepare_train(df, meme_pattern, os.path.join(path_folder_gdisk, f'{meme_pattern}.csv'))\n",
        "table = os.path.join(path_folder_gdisk, f'{meme_pattern}.csv')\n",
        "!cat \"$table\" | head -n 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9372,)\n",
            "SAID THEY HAD CORONA|I SOLVED THAT PROBLEM|\n",
            "WHOLE WORLD|THE MAN WHO ATE A BAT|\n",
            "YOU WANTED TEAMS INTEGRATION WITH JIRA?|YOU GOT IT|\n",
            "EUROPEAN UNION|POPULISTS IN THE EU|\n",
            "I BURNED DOWN YOUR HOUSE|SO YOU WOULD HAVE TO COME KEEP ME COMPANY|\n",
            "HE WAS QUARANTINED IN HIS SIDE CHICKS HOUSE||\n",
            "ME: SURELY WE HAVE AN INDEX ON COMMENTS.CREATED_AT DB001:||\n",
            "HOW MY SIBLING TELL WHAT HAPPENED|WHEN WE PLAY FIGHT|\n",
            "\"LOST 10 GAMES IN A ROW|\"\"LET'S PLAY ONE MORE\"\"|\"\n",
            "CHARI|FELL OVER ND COMMITED ARSON|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCjVlXXVIHo"
      },
      "source": [
        "Запуск дообучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfenTwWl_dqw"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_tO82mt-b_f",
        "outputId": "2784bca8-55de-4745-b26d-a4708c84eaca"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 23 20:33:00 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfjBFBH0aFQQ",
        "outputId": "ae54bc49-1b7d-4952-8386-befd6984e357"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.mount_gdrive()\n",
        "gpt2.download_gpt2(model_name=\"124M\")\n",
        "gpt2.copy_file_from_gdrive(f'{meme_pattern}.csv')\n",
        "\n",
        "history = gpt2.finetune(sess,\n",
        "        f'{meme_pattern}.csv',\n",
        "        steps=1500,\n",
        "        model_name='124M',\n",
        "        model_dir='models',\n",
        "        combine=50000,\n",
        "        batch_size=1,\n",
        "        learning_rate=0.0001,\n",
        "        accumulate_gradients=5,\n",
        "        restore_from='latest',\n",
        "        run_name=meme_pattern,\n",
        "        checkpoint_dir='checkpoint',\n",
        "        sample_every=50,\n",
        "        sample_length=100,\n",
        "        sample_num=500,\n",
        "        multi_gpu=False,\n",
        "        save_every=1000,\n",
        "        print_every=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 463Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 140Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 632Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001:   0%|                          | 0.00/498M [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 67.3Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 322Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 179Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 207Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 55.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 307396 tokens\n",
            "Training...\n",
            "[50 | 118.37] loss=1.68 avg=1.68\n",
            "OTHER|<|endoftext|>\n",
            "<|startoftext|>ME GETTING TO WATCH THE WORLD LOSE|HARD TIMING AND CANDLE OF|<|endoftext|>\n",
            "<|startoftext|>FULLY FINDED MY NEIGHBOR|WHAT A LUCK<|endoftext|>\n",
            "<|startoftext|>THEY SAID TO BURN YOUR HOUSE|SO NOW YOU CAN HAVE A HOME<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[100 | 753.89] loss=1.52 avg=1.60\n",
            "><|endoftext|>\n",
            "<|startoftext|>I STOLE THE CATCHYPE|NOW I'M SAID I'M NOT ANYMORE A SPIDER!<|endoftext|>\n",
            "<|startoftext|>WHEN YOU SEE THE RED GIRL|STAYING IN THE HOUSE<|endoftext|>\n",
            "<|startoftext|>SHE SAID \"ME AFTER THIS\" I JUST ASK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[150 | 1384.01] loss=1.52 avg=1.57\n",
            "||startoftext|<|endoftext|>\n",
            "<|startoftext|>THEY HAD CORONA|BEING ELSE TO BURY UP THE HOUSE|<|endoftext|>\n",
            "<|startoftext|>THIS IS WHAT HAPPENS|WHEN YOU DON'T START COOKING|<|endoftext|>\n",
            "<|startoftext|>YOUR BABYSITTERS|CORONAVIRUS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[200 | 2015.88] loss=1.25 avg=1.49\n",
            " B|I DON'T CARE|<|endoftext|>\n",
            "<|startoftext|>TURNING TO THE RESULT||<|endoftext|>\n",
            "<|startoftext|>I HAVE COVID-19|IT'S FINE TO KILL ME RIGHT NOW|<|endoftext|>\n",
            "<|startoftext|>WHEN THE MELTDOWN GUY BURNT YOUR HOUSE DOWN|AND YOU SAID, \"I\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[250 | 2647.82] loss=1.20 avg=1.43\n",
            "!\" I KNOW I CAN DO THIS<|endoftext|>\n",
            "<|startoftext|>THE WHOLE WORLD|IS BURNING.<|endoftext|>\n",
            "<|startoftext|>WOMEN WOUND EVERYONE|BECAUSE THEY USED TO LIVE THERE<|endoftext|>\n",
            "<|startoftext|>THERE WAS A POP-TARENT THERE|NO ONE GOT IT!<|endoftext|>\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[300 | 3284.04] loss=1.21 avg=1.39\n",
            "RAGOUSE|NOT IN MY ROOM<|endoftext|>\n",
            "<|startoftext|>WHEN THE STOCK COMPANY FIRES ALL of YOUR IP (NOTHING ON IT<|endoftext|>\n",
            "<|startoftext|>IT TOLD ME TO GO TO BED|I HAVE STABBED THE BED<|endoftext|>\n",
            "<|startoftext|>LOOK WHAT I DID|THIS WOULD\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[350 | 3916.50] loss=0.90 avg=1.32\n",
            "IDER|...|<|endoftext|>\n",
            "<|startoftext|>THIS COULD BE COLD|SO BAD|<|endoftext|>\n",
            "<|startoftext|>THE TEACHER GAVE ME AN XBOX ONE|I GAVE HER AN XBOX ONE X2|<|endoftext|>\n",
            "<|startoftext|>THEY SAID NO HOMEWORK|SO I LEFT THE HOUSE WITH A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[400 | 4546.59] loss=0.80 avg=1.25\n",
            "|TAMMY STRAIGHT|<|endoftext|>\n",
            "<|startoftext|>BARBARA AND ADAM|THE CHURCH OF LIVING AFTER THE DEBUT|<|endoftext|>\n",
            "<|startoftext|>SOMEBODY FELT|LIT MESSAGE ALLOWANT|<|endoftext|>\n",
            "<|startoftext|>WHEN YOU DON'T GET THE PICT\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[450 | 5180.78] loss=0.72 avg=1.19\n",
            " BLIES AND SAVAGE|<|endoftext|>\n",
            "<|startoftext|>I WAS JUST PREPARING|THE SAME TRAINERS|<|endoftext|>\n",
            "<|startoftext|>HALO|WHEN YOU GET THE TITLE OF KING|<|endoftext|>\n",
            "<|startoftext|>WHEN YOU BURN YOUR NIKES|BARNIES|<|endoftext|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[500 | 5814.97] loss=0.68 avg=1.14\n",
            " THE NEXT LEVEL 4 FOOTBALL AND ITS ON FIRE|<|endoftext|>\n",
            "<|startoftext|>PEARL IS NOT GOING OUT|TO GHOUL STREETS|<|endoftext|>\n",
            "<|startoftext|>THERE WAS A LIT CANDLES PUPPY IN THE HOUSE|SHE GOT WHAT WAS COMIN'|<|endoftext|>\n",
            "<|startoftext|>WH\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[550 | 6447.71] loss=0.58 avg=1.08\n",
            ">\n",
            "<|startoftext|>THERE WAS A MAN IN THE HOUSE THAT CHEATED ON HIS GIRL|THERE GONE NOW...<|endoftext|>\n",
            "<|startoftext|>THEY SHIED AWAY FROM ME... I SHIED AWAY FROM THEM...<|endoftext|>\n",
            "<|startoftext|>WHEN WE SAY BAD HA HA|YOU BETTER ATTEND AMAMOUS.COM<|end\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[600 | 7077.80] loss=0.46 avg=1.03\n",
            " BAING...|HE SHOULD WATCH WHAT U MAKE HIS OWN WAY|<|endoftext|>\n",
            "<|startoftext|>MY MOM IS IN THERE!|WHAT A CREEPY GIRL...|<|endoftext|>\n",
            "<|startoftext|>WHEN YOU MAKE AN ASSIGNMENT|I DO IT BECAUSE I LIKE IT|<|endoftext|>\n",
            "<|startoftext|>NEXT TIME|D\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[650 | 7710.98] loss=0.39 avg=0.98\n",
            "IONISING THEM TO EXERCISE|BUT THEY ARE NOT REALLY AS THOUGH AS THE ONE IN THE BARBIE DOLLS<|endoftext|>\n",
            "<|startoftext|>I SAW A SPIDER IN MY ROOM|IT IS GONE NOW<|endoftext|>\n",
            "<|startoftext|>THERE IS SOME EXPLORATION LEARNING ME|OF NOT GO TRYING TO EXERCISE DURING POLIC\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[700 | 8343.08] loss=0.31 avg=0.93\n",
            "|>\n",
            "<|startoftext|>HOBBY LOBBY|AND HE IS NOW|<|endoftext|>\n",
            "<|startoftext|>ANYTHING I SHOULD KNOW|BEFORE I TEST ON THE FLATSAT ?|<|endoftext|>\n",
            "<|startoftext|>WHEN YOUR MOM TOOK ALL YOUR|TO THE BASEMENT|<|endoftext|>\n",
            "<|startoftext|>SHE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[750 | 8974.74] loss=0.29 avg=0.88\n",
            "> I HAVE THE WHITE HOUSE<|endoftext|>\n",
            "<|startoftext|>MY TEACHER FOUND A MOUSE...|SO I BURNID IT<|endoftext|>\n",
            "<|startoftext|>I CAUSED 9/11<|endoftext|>\n",
            "<|startoftext|>WHEN YOUR TEACHER PUSHES AN IMPORTANT EXAM IN HEX|AND YOU REALIZE YOU ARE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[800 | 9608.76] loss=0.30 avg=0.84\n",
            "....I GOT SOCKS FOR CHRISTMAS FROM GRANDMA|AND THEY WANTED TO BUY ME A CHRISTMAS CHRISTMAS CHRISTIE|<|endoftext|>\n",
            "<|startoftext|>THAT LIE WAS|TO BIG FOR HIS PANTS AND BURNING PIZZA|<|endoftext|>\n",
            "<|startoftext|>LOOK! THERE WAS A SPIDER!|WITH MY MIXTAPE!|<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[850 | 10238.72] loss=0.24 avg=0.80\n",
            " I GET SICK OF THAT<|endoftext|>\n",
            "<|startoftext|>THEY TOLD ME I CAN'T HAVE A DOGE BROTHER|I TOLD THEM THEIR HOUSE WASN'T GOT ANYGIRL CRUNCH ON IT'S SIDE<|endoftext|>\n",
            "<|startoftext|>THIS IS WHAT HAPPENS|TO HATERS<|endoftext|>\n",
            "<|startoftext|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[900 | 10867.80] loss=0.20 avg=0.77\n",
            "CC>I WANTED CANDY|SO I KILLED THE CANDY|<|endoftext|>\n",
            "<|startoftext|>THEY MADE ME WATCH FROZEN AGAIN SO I KILLED THE MATCH|<|endoftext|>\n",
            "<|startoftext|>I FARTED|AND THE EARTHQUAKE WAS IN THE HOUSE|<|endoftext|>\n",
            "<|startoftext|>I\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[950 | 11500.58] loss=0.14 avg=0.73\n",
            "ANGUALS WERE ALWAYS TRYING TO GET VINCE MAKING ME DO HER FAIR HOUR|BUT THEY'RE ALL GETTING NO REASON NOW<|endoftext|>\n",
            "<|startoftext|>MY BEST FRIEND SAID HE COULDN'T FIND THE SPIDER... SO I TURNED HIM UP!<|endoftext|>\n",
            "<|startoftext|>HEY LITTLE GIRL, WHAT CAN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1000 | 12127.90] loss=0.15 avg=0.70\n",
            "Saving checkpoint/disaster girl/model-1000\n",
            ">\n",
            "<|startoftext|>DREADS ASSEMBLY||<|endoftext|>\n",
            "<|startoftext|>MUM SAID WE COULDN'T HAVE ANY DESSERT|SO..I TOLD HIM I LIT A FIRE AND SET THE HOUSE ON FIRE|<|endoftext|>\n",
            "<|startoftext|>BLACKLIVES MATTER MORE THAN ECONOMY|WALMART<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1050 | 12764.66] loss=0.09 avg=0.67\n",
            "text|>NEVER MADE AN INTERIOR ATTACK WITHOUT STAR BAKING|ME!|<|endoftext|>\n",
            "<|startoftext|>THE FIRST DAY OF SHIP STAYS UNLIVED. THE REST OF THE WORLD|CORONAVIRUS|<|endoftext|>\n",
            "<|startoftext|>WANTED: MACCIA|DAMTRY NIEDER DEAD|REWARD:|<|endoftext\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1100 | 13395.64] loss=0.08 avg=0.64\n",
            "M|<|endoftext|>\n",
            "<|startoftext|>THE WORLD|BEN, ILIL, YARDEL, JOSH, AND JOSH ENGAGE TEAM|<|endoftext|>\n",
            "<|startoftext|>WHEN YOUR MOM SAYS NO FORTNITE|HER HOUSE BURNS DOWN|<|endoftext|>\n",
            "<|startoftext|>OB AND LIS BAND|DONT KEEP THE BO\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1150 | 14023.43] loss=0.05 avg=0.61\n",
            ">\n",
            "<|startoftext|>THEY WANTED S'MORES|I MADE THEM S'MORES<|endoftext|>\n",
            "<|startoftext|>AND THEN YOU|FOUND OUT WHERE THE PLAYSTATION QUNSTCHAT HAD BEEN AND SAID \"TRICK OR TREAT\"<|endoftext|>\n",
            "<|startoftext|>I LOST A GAME|BECAUSE THE AI WASNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1200 | 14656.68] loss=0.07 avg=0.58\n",
            "|endoftext|>\n",
            "<|startoftext|>MY MOM COOKED ME A POST<|endoftext|>\n",
            "<|startoftext|>EAT THE LAST DRY COOKIE I DIE<|endoftext|>\n",
            "<|startoftext|>WHEN YOUR DOG IS GROUNDED YOUR PLAYDATE WAS TAKEN AWAY<|endoftext|>\n",
            "<|startoftext|>REMEMBER\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1250 | 15285.52] loss=0.06 avg=0.56\n",
            "startoftext|><|startoftext|>OH I FOUND DORY ALRIGHT|LOOKS LIKE HE CAN NOW|<|endoftext|>\n",
            "<|startoftext|>ALL THEY HAD TO DO WAS|GIVE ME THAT TOY, AND SMILE TOO|<|endoftext|>\n",
            "<|startoftext|>THE ONLY WAY TO STOP BEING HAVING BED FOR EVER|BROCCOL\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1300 | 15914.11] loss=0.04 avg=0.54\n",
            "|startoftext|>THE FACE I MAKE WHEN|WHEN IM TOLD TO PICK A PARTNER<|endoftext|>\n",
            "<|startoftext|>TYLER FOR HALLOWEEN|ANOTHER GIRL SColded ME FOR BEING AMAZED AT MY BARBIE DINER<|endoftext|>\n",
            "<|startoftext|>AT LEAST I MADE|GRILLED CHEESE<|end\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1350 | 16563.31] loss=0.03 avg=0.52\n",
            "<|startoftext|>MY BROTHER TOUCHED MY|BARBIE DOLLS AND PC GOT BURNT ME|<|endoftext|>\n",
            "<|startoftext|>THEY TOLD ME TO BUY A PONY|I DID DAT MY HOMEWORK<|endoftext|>\n",
            "<|startoftext|>HOW IT STARTED|BURNED THE HOUSE DOWN<|endoftext|>\n",
            "<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1400 | 17226.38] loss=0.05 avg=0.50\n",
            "ES|<|endoftext|>\n",
            "<|startoftext|>WHEN MOMMY SAYS|YOU ARE DEMENTPHYXED AND HERD BLOOD ALCOHOL POQUETFOLLAS|<|endoftext|>\n",
            "<|startoftext|>WORLD|COVID 19|<|endoftext|>\n",
            "<|startoftext|>FOPPAS SERVER|LE CALLIS|<|endof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1450 | 17893.35] loss=0.05 avg=0.48\n",
            " HIS REAL IDENTITY<|endoftext|>\n",
            "<|startoftext|>THERE WAS A CONVERSATION POTIED|IT WAS UNDERGROUND<|endoftext|>\n",
            "<|startoftext|>THEY SAID WE NEEDED TO RE-THINK|MY MOM'S EXPERIENCE<|endoftext|>\n",
            "<|startoftext|>\"IF THIS HOUSE BELIEVES IN GHOST 3|HE SHOULD NEVER\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1500 | 18561.68] loss=0.06 avg=0.46\n",
            "Saving checkpoint/disaster girl/model-1500\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EI-HwsAN_YXX"
      },
      "source": [
        "with open(f'/gdrive/{meme_pattern}_history.pickle', 'wb') as f:\n",
        "  pickle.dump(history, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UpoKaAHNGzy"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=meme_pattern)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}