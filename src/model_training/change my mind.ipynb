{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dec36z4GYpRe"
      },
      "source": [
        "# Настройка доступов и подгрузка библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHRMYs8D-kVi"
      },
      "source": [
        "meme_pattern = 'change my mind'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMcA-r-JYwhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b3d782-642e-4ac7-9d2f-71f2bfb6d5e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo0knl7RbaP7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igcl3kHaCZY5"
      },
      "source": [
        "!ln -s \"/content/drive/My Drive\" /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "733q7zPFaBSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4f193f-2c41-445b-b833-2086131e8c77"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install git+https://github.com/SerSmith/gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting git+https://github.com/SerSmith/gpt-2-simple\n",
            "  Cloning https://github.com/SerSmith/gpt-2-simple to /tmp/pip-req-build-ldf8mgw4\n",
            "  Running command git clone -q https://github.com/SerSmith/gpt-2-simple /tmp/pip-req-build-ldf8mgw4\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.18.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple==0.7.1) (0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple==0.7.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->gpt-2-simple==0.7.1) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->gpt-2-simple==0.7.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->gpt-2-simple==0.7.1) (0.17.0)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=25644 sha256=0ac6ce01277f93d002a1f81bd10684958f5fcf564405e66923435c5bb21c69cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7tahxdm6/wheels/05/e5/9d/9b6ebf15d32922867291a66ba17b8610a5621e6cd1a399fba5\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scJH4yBGUKTn"
      },
      "source": [
        "# Подготовка датасета для дообучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWJvqxY11oFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd78068a-c7f1-44b0-8131-8155ccb35757"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "import shutil\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "path_raw_data = '/gdrive/MADE_PROJECT/raw_data/memes_all_eng_v2.csv'\n",
        "path_folder_gdisk = '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS6A3cQATkdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977302e1-f854-4701-b684-f000c11e4ca8"
      },
      "source": [
        "df = pd.read_csv(path_raw_data)\n",
        "df['template_name_prep'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "batman slapping robin    18241\n",
              "x x everywhere           16677\n",
              "waiting skeleton         12314\n",
              "change my mind           12169\n",
              "disaster girl             9418\n",
              "Name: template_name_prep, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7rtIwaDdqUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b2636d-4e99-418d-e893-fd8ce81a4bb9"
      },
      "source": [
        "def prepare_text(text):\n",
        "  text = str(text).strip().upper()\n",
        "  text = re.sub(r\"\\s\", \" \", text)\n",
        "  text = re.sub(r\"\\n\", \" \", text)\n",
        "  text = re.sub(r\"&\", \"and\", text)\n",
        "  text = re.sub(r\"|\", \"\", text)\n",
        "  return text\n",
        "\n",
        "def prepare_train(data, template, dest_path):\n",
        "  out = df[df['template_name_prep'] == template]['text'].drop_duplicates().reset_index(drop=True).apply(prepare_text)\n",
        "  print(out.shape)\n",
        "  out.to_csv(dest_path, index=False, header=False)\n",
        "  return out\n",
        "\n",
        "q = prepare_train(df, meme_pattern, os.path.join(path_folder_gdisk, f'{meme_pattern}.csv'))\n",
        "table = os.path.join(path_folder_gdisk, f'{meme_pattern}.csv')\n",
        "!cat \"$table\" | head -n 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12034,)\n",
            "EVERY DAY WITH WENDY IS A WEN-DAY||\n",
            "CHELKAT ARE ONLY FRIENDS WITH MEGAN SO THEY HAVE SOMEONE TO BE MEAN TO||\n",
            "WORKING FROM HOME ISN'T AS AWESOME AS WE ANTICIPATED||\n",
            "KEVIN SHOULD BE ALLOWED BACK IN||\n",
            "\"CALLING ME \"\"PARANOID\"\" DOESN'T PROVE YOU'RE NOT OUT TO GET ME.||\"\n",
            "CHARLIE BAKER IS THE BIGGEST PUSSY IN POLITICS.||\n",
            "WASH YOUR HANDS OR END UP LIKE SM||\n",
            "CAN ANYBODY|FEEL ME OUT THERE|\n",
            "\"I DON'T NEED HELP, I'M ENOUGH AS IS|BEOWULF|\"\n",
            "COVID-19 IS A GENETICALLY ENGINEERED BIOWEAPON||\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCjVlXXVIHo"
      },
      "source": [
        "Запуск дообучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfenTwWl_dqw"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_tO82mt-b_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f75fd3-32a7-4e5b-fd6f-d8fdea9382a9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov 24 15:50:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfjBFBH0aFQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08fab36-1d05-4378-8920-51c8805bfa29"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.mount_gdrive()\n",
        "gpt2.download_gpt2(model_name=\"124M\")\n",
        "gpt2.copy_file_from_gdrive(f'{meme_pattern}.csv')\n",
        "\n",
        "history = gpt2.finetune(sess,\n",
        "        f'{meme_pattern}.csv',\n",
        "        steps=2500,\n",
        "        model_name='124M',\n",
        "        model_dir='models',\n",
        "        combine=50000,\n",
        "        batch_size=1,\n",
        "        learning_rate=0.0001,\n",
        "        accumulate_gradients=5,\n",
        "        restore_from='latest',\n",
        "        run_name=meme_pattern,\n",
        "        checkpoint_dir='checkpoint',\n",
        "        sample_every=50,\n",
        "        sample_length=100,\n",
        "        sample_num=500,\n",
        "        multi_gpu=False,\n",
        "        save_every=1000,\n",
        "        print_every=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 379Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 132Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 397Mit/s]                                                    "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:01, 275Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 272Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 172Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 138Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 72.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 375790 tokens\n",
            "Training...\n",
            "[50 | 120.96] loss=1.75 avg=1.75\n",
            "| |<|startoftext|>WIKIMES STILL PICK UP ALL WE NEED, SO WE WILL GET THERE<|endoftext|>\n",
            "<|startoftext|>JUICE CIRCLES ARE UNHAPPIERCUTELY GOOD FOR YOUR COLD MIND<|endoftext|>\n",
            "<|startoftext|>MIRACLE DURING OUR NEXT HOMESTEAD IS A POTATO THAT WAS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[100 | 801.51] loss=1.60 avg=1.67\n",
            "ENITIVE\" IS ACTUALLY A BAD THING AS IF YOU DONT KNOW IT ALL AND CAN'T SEE IT ALL. IT'S JUST STUPID BECAUSE YOU CAN SEE THAT THEIR WANDER IS ACTUALIZED BUT IT'S NOT IMPORTANT... AND... THEY JUST CUT THE POINT OF THE GAME INTO A HISTORY OF USING A WORD FOR THE FIRST TIME. I'M LOUD AND HUMAN TO SEE THAT IT'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[150 | 1453.58] loss=1.49 avg=1.61\n",
            "ITY ARE MADE OF COSTUME AND DRY DRY LAP<|endoftext|>\n",
            "<|startoftext|>WHEN YOU GO TO WORK IT IS STOCK AND YOU SLEEP LASERS<|endoftext|>\n",
            "<|startoftext|>SOMETHING LIKE \"THESE KITCHEN IS OVERRATED AND BORN IN BRAZIL<|endoftext|>\n",
            "<|startoftext|>M\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[200 | 2114.82] loss=1.18 avg=1.50\n",
            "ofof|<|endoftext|>\n",
            "<|startoftext|>YOUR NUDES ARE THE BEST|TOXIC KID|<|endoftext|>\n",
            "<|startoftext|>I'M THE BADEST BIRTHDAY SON BY THE DRAMA TOS||<|endoftext|>\n",
            "<|startoftext|>THIS PLACE IS BAD FOR THE CITIZENS||<|endoftext|>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[250 | 2766.86] loss=1.28 avg=1.46\n",
            "ARTY AND HIS PIZZA...|HE IS RIGHT|<|endoftext|>\n",
            "<|startoftext|>CHICKEN FROSTES ARE BETTER THAN CAKE||<|endoftext|>\n",
            "<|startoftext|>CALIFORNIA BED RATS IS THE BEST JUDY BROWN SHOW||<|endoftext|>\n",
            "<|startoftext|>MEMES SHOULD BE IN THE BOOK.||\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[300 | 3437.85] loss=1.33 avg=1.44\n",
            "|>\n",
            "<|startoftext|>GOD ISN'T IMPOSSIBLE<|endoftext|>\n",
            "<|startoftext|>WE SHOULD ALL BE BISSILE MEMES ON IMAGE MODE<|endoftext|>\n",
            "<|startoftext|>GAMERS HAVE A BAD GIRL MEME TRACKING. TRY EVERYTHING YOUF<|endoftext|>\n",
            "<|startoftext|>S\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[350 | 4104.63] loss=1.09 avg=1.39\n",
            "-of-course||<|endoftext|>\n",
            "<|startoftext|>THE CORONA VIRUS WAS NOT A GOOD IDEA.<|endoftext|>\n",
            "<|startoftext|>IF THE EARTH IS A PLANET. AND THE TROUBLE THE EARTH WAS CREATED IN. AND NOT PLANET AND EARTH<|endoftext|>\n",
            "<|startoftext|>THEY STARTED THE CORONAVIR\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[400 | 4775.26] loss=0.96 avg=1.33\n",
            " TO BATTLEGROUND<|endoftext|>\n",
            "<|startoftext|>TOM HAS NAMES THAT IS NOT HIS REAL NAME<|endoftext|>\n",
            "<|startoftext|>MEMERS GO BASTARDY FOR DOING THIS<|endoftext|>\n",
            "<|startoftext|>THIS MEME WILL HAVE 10,000 VIEWS<|endoftext|>\n",
            "<|startoftext|>THE EARTH S\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[450 | 5454.70] loss=1.03 avg=1.30\n",
            "||END OF DBZ GOKU MS01||<|endoftext|>\n",
            "<|startoftext|>SUMO'S DON'T DO THEIR WORK SO THEY WILL|JOHNNY|<|endoftext|>\n",
            "<|startoftext|>THE WALKING DEAD IS A JOKE|A BUNCH OF DOLLARS|<|endoftext|>\n",
            "<|startoftext|>BECAUSE OF SOMETH\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[500 | 6136.82] loss=0.94 avg=1.26\n",
            "IM HELLO IT WAS ME.|HELLO IT OM|<|endoftext>\n",
            "<|startoftext>FALL RERUNS ARE JUST HIGHSIZED SHRIMP CHINESE DISPOSAL.||<|endoftext>\n",
            "<|startoftext>THERE'S NO ALGORITHM TO SUE FOR THE WORLD TO HEARING||<|endoftext>\n",
            "<|startoftext>THE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[550 | 6821.54] loss=0.68 avg=1.20\n",
            " THE EARTH IS ROUND<|endoftext|>\n",
            "<|startoftext|>PEARL JAM IS TOPGIT<|endoftext|>\n",
            "<|startoftext|>THERE IS ONLY ONE JISHI JUDA JIHU.<|endoftext|>\n",
            "<|startoftext|>EASTER GIRL WEARING A CUTE FINGER IS NOT A CAREER.<|endoftext|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[600 | 7494.98] loss=0.65 avg=1.15\n",
            "ANES||<|endoftext|>\n",
            "<|startoftext|>STAYCATION SCHOOL SHOULD HAVE NAP JUICE.||<|endoftext|>\n",
            "<|startoftext|>IF I STEP OUTSIDE FOR ONE DAY, I WILL GET FIRED||<|endoftext|>\n",
            "<|startoftext|>MOSQUITOES ARE VAMPIRE RESTRICTIONS ON PUBLIC WORKFORCE.||<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[650 | 8160.58] loss=0.60 avg=1.11\n",
            "OST JUST IN CASE IT WAS ALL WRONG<|endoftext|>\n",
            "<|startoftext|>GIVE ME V BUCKS<|endoftext|>\n",
            "<|startoftext|>THERE ARE NO AMERICAN NORMAL MEMES<|endoftext|>\n",
            "<|startoftext|>THE NEW SPEAKER IS JUST A MOTO GAE RANGE<|endoftext|>\n",
            "<|startoftext|>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[700 | 8813.02] loss=0.54 avg=1.07\n",
            "text|>MILK IS MUSHROOM WITH A DEAD ARMY<|endoftext|>\n",
            "<|startoftext|>THIS TEXT IS FART STRAIGHT<|endoftext|>\n",
            "<|startoftext|>TRAPS ARE NOT GAY<|endoftext|>\n",
            "<|startoftext|>CORONAVIRUS IS JUST BOOMER REMOVER<|endoftext|>\n",
            "<|startoftext|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[750 | 9469.91] loss=0.55 avg=1.03\n",
            " BE IS A GOOD IDEA<|endoftext|>\n",
            "<|startoftext|>WE ARE HOMELESS BECAUSE WE LIVE IN A FEW PLANETS CLUE<|endoftext|>\n",
            "<|startoftext|>PASTA IS A MENTALLY RETARDED COLD<|endoftext|>\n",
            "<|startoftext|>WHEN YOU GET A JOB OFFER FROM A COMPANY THAT YOU DOESNT G\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[800 | 10128.79] loss=0.47 avg=0.99\n",
            " FIRE<|endoftext|>\n",
            "<|startoftext|>THE GOLDBERGS IS JUST A PLACE FOR U COUGH UPVOTES AND EAT EMOJIS<|endoftext|>\n",
            "<|startoftext|>YOU DONT KNOW WHAT MY MUG SAYS<|endoftext|>\n",
            "<|startoftext|>I USE MATH EVERY SINGLE DAY. USE ME.<|endoftext|>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[850 | 10784.75] loss=0.29 avg=0.95\n",
            "ARAL<|endoftext|>\n",
            "<|startoftext|>PEOPLE WHO REPOST ARE IN NEED OF ANIMAL CROSSING<|endoftext|>\n",
            "<|startoftext|>IF WW3 DOSENT KILL US ALL CHORONAVIRUS WILL<|endoftext|>\n",
            "<|startoftext|>THE BRAIN IS THE MOST IMPORTANT ORGANS ACCORDING TO THE BRAIN<|endof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[900 | 11444.83] loss=0.35 avg=0.91\n",
            "<|startoftext|>YOGA PANTS ARE THIS GENERATION'S SWEAT PANTS.<|endoftext|>\n",
            "<|startoftext|>CHOCOLATE ICE CREAM IS THE BEST.<|endoftext|>\n",
            "<|startoftext|>HALF OF THE MEMES ON IMGFLIP DONT MAKE SENSE<|endoftext|>\n",
            "<|startoftext|>MEMES SHOULD BE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[950 | 12095.16] loss=0.33 avg=0.88\n",
            " IS TOILET SELLOUT<|endoftext|>\n",
            "<|startoftext|>FALLOUT 4EVER. I'LL KILL YOU<|endoftext|>\n",
            "<|startoftext|>SPOOKTING DIES. GROOM THANOS WIN<|endoftext|>\n",
            "<|startoftext|>THIS IS HOW I GET JOYS ON IMGFLIP<|endoftext|>\n",
            "<|start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1000 | 12745.67] loss=0.24 avg=0.84\n",
            "Saving checkpoint/change my mind/model-1000\n",
            "oftext|>\n",
            "<|startoftext|>GOUVERNEMENT MEMES HAVE GOTTEN A 10 ON A BUNCH OF PEOPLE, APPLYING JUST HALF AS LARGE CAPSULE.<|endoftext|>\n",
            "<|startoftext|>YOU CAN'T HAVE HALF A CERTAIN DECADE IN AUSTRALIA<|endoftext|>\n",
            "<|startoftext|>BEES ARE JUST CAKE WHEAT\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1050 | 13391.68] loss=0.27 avg=0.81\n",
            "NAIC COOKIE IS JUST A LOAF OF MILK||<|endoftext|>\n",
            "<|startoftext|>NO ONE WILL UPVOTE THIS NOTUNE||<|endoftext|>\n",
            "<|startoftext|>|SOME OF YOU ARE POWERFUL AND UNLOAVED UPVOTES. THIS IS REALLY ME BEATING||<|endoftext|>\n",
            "<|startoftext|>I MAKE COOL MEMES\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1100 | 14036.99] loss=0.21 avg=0.78\n",
            "TEU HIRSCH<|endoftext|>\n",
            "<|startoftext|>EVERY VEGETABLE VEGETABE IN PUIET PLATE, YOU CAN'T CHANGE MY MIND<|endoftext|>\n",
            "<|startoftext|>GRAHAM WAVES HIS HANDS AROUND TOO MUCH DURING HEARING TREES SHOWS<|endoftext|>\n",
            "<|startof\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1150 | 14685.10] loss=0.15 avg=0.75\n",
            "<|startoftext|>A BULLET IS A BACON WITH A COLON. IT IS A BACON<|endoftext|>\n",
            "<|startoftext|>MAKE ME<|endoftext|>\n",
            "<|startoftext|>SWEDEN IS DOING RIGHT, OLD PEOPLE ARE DOING WESTERN. SWEDEN IS DOING THE RIGHT WAY.<|endoftext|>\n",
            "<|startoftext|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1200 | 15331.27] loss=0.15 avg=0.72\n",
            "||>\n",
            "<|startoftext|>NO ONE WILL SIT WITH ME<|endoftext|>\n",
            "<|startoftext|>A LAMBORGHINI IS A CRINGE<|endoftext|>\n",
            "<|startoftext|>BARNEY IS TO TUCKER AND WHATCHED MEETINGS<|endoftext|>\n",
            "<|startoftext|>THIS IS SPARTA<|endoftext|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1250 | 15973.78] loss=0.16 avg=0.70\n",
            "<|endoftext|>\n",
            "<|startoftext|>WEEBS OF 100% AREA 51 WILL STAR AT THE TOP||<|endoftext|>\n",
            "<|startoftext|>CAROL TOTALLY FED HER HUSBAND TO TIGERS||<|endoftext|>\n",
            "<|startoftext|>THE CIA STOOD AT KEY WORKERS AND IT HAPPENS TODAY||<|endoftext|>\n",
            "<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1300 | 16612.52] loss=0.13 avg=0.67\n",
            "startoftext|>FANART IS JUST REPUBLICANS WANTING TO LEAVE THEIR JOBS TO COVID-19||<|endoftext|>\n",
            "<|startoftext|>CARLY SLAYER IS A BOB<|endoftext|>\n",
            "<|startoftext|>CONSIDER PAINING THE SOY FUN STREAM HERE.||<|endoftext|>\n",
            "<|startoftext|>WHEN YOU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1350 | 17248.68] loss=0.11 avg=0.65\n",
            "|>THE NEW SONIC WAS CREATED BY A., DID NOT CONTRACT TO MAKE INHABITATION WITH THE EXPENDABLE:.)<|endoftext|>\n",
            "<|startoftext|>I AM SEXY<|endoftext|>\n",
            "<|startoftext|>DRAGON BALL ANIMATION ISN'T REAL<|endoftext|>\n",
            "<|startoftext|>DOGS ARE THE MOST\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1400 | 17894.00] loss=0.07 avg=0.63\n",
            "ADDISH, WON'T STOP THE JERK GANG FROM KILLING ME<|endoftext|>\n",
            "<|startoftext|>I EAT VEAL IN MINECRAFT BEFORE YA<|endoftext|>\n",
            "<|startoftext|>I'M TELETRUSTING MY FATHER. SMOKING INTO A CHICKEN<|endoftext|>\n",
            "<|startoftext|>GEORGE LAZIG\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1450 | 18537.02] loss=0.08 avg=0.60\n",
            "WINDINGS<|endoftext|>\n",
            "<|startoftext|>THE REASON VACCINATED KIDS WALK THE FAST IS BECAUSE THEY REQUIRE CURRENT BRAIN TRA BRAINING<|endoftext|>\n",
            "<|startoftext|>PREGNANT WOMEN ARE NICE<|endoftext|>\n",
            "<|startoftext|>THE HARDEST HIT FINANCIALLY DURING THE QUARANT\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1500 | 19176.03] loss=0.07 avg=0.58\n",
            "\n",
            "|STEVEN WEISSENBERG TOLLOW UNHOLY PAPER||<|endoftext|>\n",
            "<|startoftext|>FREDDIE MERCURY IS THE OPPOSITE OF DOG EAT PSYCHOTROPIC EXPERT||<|endoftext|>\n",
            "<|startoftext|>ONE BY ED SHEERAN IS A GREAT SONG||<|endoftext|>\n",
            "<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.true_divide(self.todense(), other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1550 | 19815.72] loss=0.05 avg=0.56\n",
            "KG IS JUST RACIST<|endoftext|>\n",
            "<|startoftext|>PEOPLE WHO ARGUE JUST TO ARGUE ARE UNAGGRY<|endoftext|>\n",
            "<|startoftext|>DOLPHINS ARE JUST UNICORNS<|endoftext|>\n",
            "<|startoftext|>UNDERTALE IS CRAP. CUSTOM ZONE<|endoftext|>\n",
            "<|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1600 | 20449.31] loss=0.05 avg=0.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI-HwsAN_YXX"
      },
      "source": [
        "with open(f'/gdrive/{meme_pattern}_history.pickle', 'wb') as f:\n",
        "  pickle.dump(history, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UpoKaAHNGzy"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=meme_pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoqIO5KSYR_W"
      },
      "source": [
        "# Генерация текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvbn1BvJYY39"
      },
      "source": [
        "# sess = gpt2.start_tf_sess()\n",
        "# gpt2.load_gpt2(sess, run_name='disaster girl 12 11')\n",
        "# gpt2.generate(sess,\n",
        "#               length=100,\n",
        "#               temperature=0.7,\n",
        "#               prefix=\"Change my mind\",\n",
        "#               nsamples=5,\n",
        "#               batch_size=5,\n",
        "#               truncate='EOF'\n",
        "#               )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}